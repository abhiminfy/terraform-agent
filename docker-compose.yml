services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  api:
    build:
      context: .
      dockerfile: src/Dockerfile
    image: tfagent/api:latest
    working_dir: /app
    command: uvicorn --host 0.0.0.0 --port 8000 src.main:app
    env_file:
      - .env
    environment:
      # let both "src.*" and "app.*" imports resolve
      PYTHONPATH: /app:/app/src
      # make API and worker share the same broker/result backend
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
    depends_on:
      - redis
    ports:
      - "8000:8000"
    restart: unless-stopped

  worker:
    build:
      context: .
      dockerfile: src/Dockerfile
    image: tfagent/api:latest
    working_dir: /app
    command: celery -A src.app.services.background.celery_app worker --loglevel=INFO
    env_file:
      - .env
    environment:
      PYTHONPATH: /app:/app/src
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
    depends_on:
      - redis
    restart: unless-stopped

  webui:
    image: ghcr.io/open-webui/open-webui:0.6.30
    ports:
      - "3000:8080"
    environment:
      OPENAI_API_BASE_URL: http://api:8000/v1   # points WebUI to your FastAPI
      OPENAI_API_KEY: "dummy"                   # anything non-empty
      ENABLE_OPENAI_API: "true"                 # auto-enable OpenAI provider
      # Optional for local testing:
      # WEBUI_AUTH: "false"
    depends_on:
      - api
    volumes:
      - webui:/app/backend/data                 # <-- persist provider/settings DB
    restart: unless-stopped

volumes:
  webui:
